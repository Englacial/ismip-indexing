{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3470624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import cftime\n",
    "\n",
    "import ismip6_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f58be",
   "metadata": {},
   "source": [
    "## Loading ISMIP6 Antarctica outputs\n",
    "\n",
    "* ISMIP6 Antarctica outputs are ~1.1 TB in total. Officially, they are available through Globus, but we've pulled the whole dataset and put it on GCloud at `gs://ismip6/` (byte-for-byte identical -- we don't want to be responsible for hosting modified versions of things like this).\n",
    "* Following CMIP conventions, every variable is a separate NetCDF file. Nominally, these are CF-compliant and follow a standardized set of file and variable naming rules, but, following CMIP conventions ðŸ™‚, there are a scattering of errors. See the [ISMIP6 output specifications](https://theghub.org/groups/ismip6/wiki/MainPage/ISMIP6ProjectionsAntarctica).\n",
    "* All of the outputs are uniform rectangular grids in EPSG:3031 projection, but there are multiple resolutions.\n",
    "\n",
    "\n",
    "Ideally, lazy loading of this dataset should be easy and concise. We would like loading a dataset like this into an Xarray DataTree to be a two line operation:\n",
    "\n",
    "```python\n",
    "catalog = helper_library.open(\"gs://lightweight-reference-file\")\n",
    "# If desired, filter the catalog\n",
    "dt = catalog.to_datatree()\n",
    "```\n",
    "\n",
    "Where there are two important properties we care about here:\n",
    "\n",
    "1. We want to be able to create `lightweight-reference-file` for an existing dataset without needing to change the underlying bytes\n",
    "2. We want to be able to encode \"fixes\" somewhere before it becomes an Xarray DataTree -- \"fixes\" are things like different variable naming conventions, misspelled files, incorrect time axes, etc.\n",
    "\n",
    "The code below actually loads the ISMIP6 outputs into a DataTree. It's a bit more than two lines.\n",
    "\n",
    "The main issues are inconsistencies in the output files. For example:\n",
    "* A few files are mis-named (missing an underscore) â†’ corrected by `ismip6_helper.get_file_index()`\n",
    "* Some grids were defined with `x` and `y` coordinates (in EPSG:3031 projection) while others were specified by `lat`, `lon` points â†’ corrected by `ismip6_helper.correct_grid_coordinates()`\n",
    "* Timestamps are specified in a variety of different formats with various CF-compliance issues â†’ fixed by `ismip6_helper.open_ismip6_dataset()` which automatically detects and corrects:\n",
    "  - Typo: `unit` instead of `units` attribute\n",
    "  - Invalid use `MM-DD-YYYY` instead of `YYYY-MM-DD`\n",
    "  - Invalid dates (e.g., day 0 â†’ day 1)\n",
    "  - etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe index by scanning the ISMIP6 Antarctica output files\n",
    "# This is ~200 lines of code to build an index of ISMIP6 data files from the filenames\n",
    "ismip6_df = ismip6_helper.get_file_index()\n",
    "\n",
    "# For the purposes of this demo, filter down the number of files we have to load\n",
    "ismip6_df = ismip6_df.query('experiment in [\"ctrl_proj_std\", \"exp05\", \"ctrl_proj\"] and variable in [\"lithk\", \"base\", \"sftgrf\"] and institution in [\"JPL1\", \"AWI\", \"DOE\"]')\n",
    "\n",
    "# Build a DataTree of the outputs\n",
    "datasets = {}\n",
    "for _, row in ismip6_df.iterrows():\n",
    "    try:\n",
    "        p = f'{row[\"institution\"]}_{row[\"model_name\"]}/{row[\"experiment\"]}/{row[\"variable\"]}' # DataTree path\n",
    "        \n",
    "        # Use the new helper function that automatically fixes time encoding issues\n",
    "        ds = ismip6_helper.open_ismip6_dataset(row[\"url\"], chunks={'time': 1})\n",
    "        ds = ismip6_helper.correct_grid_coordinates(ds, data_var=row[\"variable\"])\n",
    "\n",
    "        datasets[p] = ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {p}: {e}\")\n",
    "        \n",
    "ismip6_dt = xr.DataTree.from_dict(datasets)\n",
    "\n",
    "#ismip6_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f132d",
   "metadata": {},
   "source": [
    "### Select and plot one variable\n",
    "\n",
    "Once we have the DataTree loaded, we can easily filter down to variables of interest: `ismip6_dt['JPL1_ISSM']['exp05']['lithk']`\n",
    "\n",
    "**This part works well enough.**\n",
    "\n",
    "The example below produces a plot of the change in ie thickness since the beginning of the simulation. So far, we've only lazily loaded the data, so the actual data hasn't been downloaded. We call `.compute()` on the thickness change variable to force loading of the data in order to make the interactive plot responsive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedaf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one dataset from the DataTree\n",
    "dt = ismip6_dt['JPL1_ISSM']['exp05']['lithk']\n",
    "\n",
    "# Compute the change in thickness relative to the first time step\n",
    "# Since the datasets are lazily loaded, we now want to actually force computation of a result\n",
    "# so that the interactive plot will be responsive.\n",
    "delta_thickness = (dt['lithk'] - dt['lithk'].isel(time=0)).rename('delta_lithk').compute()\n",
    "\n",
    "# Determine a useful color scale range\n",
    "vmag = np.max(np.abs(delta_thickness.quantile([0.01, 0.99]).values))\n",
    "\n",
    "# Plot with a slider to change the date\n",
    "delta_thickness.hvplot.image(x='x', y='y', clim=(-vmag, vmag), cmap='RdBu').opts(\n",
    "        aspect='equal',\n",
    "        title=\"Change in ice thickness relative to the first timestep\",\n",
    "        colorbar_opts={'title': 'Change in thickness (m)'},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebade53",
   "metadata": {},
   "source": [
    "### Regridding multiple models to a common comparison grid\n",
    "\n",
    "While all of the ISMIP6 outputs were interpolated to a regular grid, these grids have different resolutions. So if we want to do any cross-model comparison, we need to get things onto a common grid.\n",
    "\n",
    "We have some more complicated ideas about how to do regridding, but we also want to make sure that simple things work.\n",
    "\n",
    "Ideally, it would be possible to call `interp` on a DataTree like this:\n",
    "\n",
    "```python\n",
    "comparison_grid = xr.Dataset({\n",
    "    'x': (['x'], np.arange(-3040e3, 3040e3, 16e3)),\n",
    "    'y': (['y'], np.arange(-3040e3, 3040e3, 16e3)),\n",
    "    'time': (['time'], xr.date_range('2016-01-01', '2100-12-31', freq='10YS').values),\n",
    "})\n",
    "\n",
    "ismip6_dt_regridded = ismip6_dt.interp(x=comparison_grid.x, y=comparison_grid.y)\n",
    "```\n",
    "\n",
    "This doesn't actually work yet, but we can use `map_over_datasets` to do the same thing. Not terrible, but could be cleaner.\n",
    "\n",
    "The other issue is that the CMIP standard (inherited by ISMIP) is one variable per file. This leads to the following structure:\n",
    "\n",
    "```\n",
    "model\n",
    " |-> var1\n",
    " |-> var2\n",
    "model2\n",
    " |-> var1\n",
    " |-> var2\n",
    "```\n",
    "\n",
    "This makes it hard to manipulate variables together because map_over_datasets doesnâ€™t easily give access to children of the nodes. So instead we manually condense the leaves of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_grid = xr.Dataset({\n",
    "    'x': (['x'], np.arange(-3040e3, 3040e3, 16e3)),\n",
    "    'y': (['y'], np.arange(-3040e3, 3040e3, 16e3)),\n",
    "    'time': (['time'], xr.date_range('2016-01-01', '2100-12-31', freq='10YS').values),\n",
    "})\n",
    "\n",
    "regridded = ismip6_dt.map_over_datasets(\n",
    "    lambda x: x.interp(\n",
    "        x=comparison_grid.x,\n",
    "        y=comparison_grid.y,\n",
    "        time=comparison_grid.time,\n",
    "        method='linear'\n",
    "    ) if ('x' in x.dims and 'y' in x.dims and 'time' in x.dims) else x\n",
    ")\n",
    "\n",
    "def condense_datatree(dt):\n",
    "    dt = dt.copy()\n",
    "    if dt.is_hollow and all(child.is_leaf for child in dt.children.values()):\n",
    "        # TODO: Should actually check that the dimensions are compatible\n",
    "        return xr.merge([child.ds for child in dt.children.values()])\n",
    "    for child_name, child in dt.children.items():\n",
    "        dt[child_name] = condense_datatree(child)\n",
    "    return dt\n",
    "\n",
    "regridded = condense_datatree(regridded)\n",
    "ismip6_dt_condensed = condense_datatree(ismip6_dt) # Also condense the non-regridded for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07c612",
   "metadata": {},
   "source": [
    "Now that we're working on a common comparison grid, we can do some cross-model comparison. As an example, we'll plot the standard deviation of the change in ice thickness since the first timestep of each model.\n",
    "\n",
    "This works but it's a bit ugly, mostly because our list comprehension has to test if we're on the right node level. Possibly a better solution might look like:\n",
    "\n",
    "```python\n",
    "ismip6_dt.match['*/exp05']['lithk'].std()\n",
    "```\n",
    "\n",
    "Or just\n",
    "\n",
    "```python\n",
    "xr.concat(regridded.match(\"*/*\"), dim='model').std(dim='model')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd583670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of the change in lithk across models\n",
    "delta_lithk_all = xr.concat([\n",
    "    (node.ds['lithk'].isel(time=slice(1, None)) - node.ds['lithk'].isel(time=0)) \n",
    "    for node in regridded.subtree \n",
    "    if node.path.endswith('exp05') and node.has_data\n",
    "], dim='model').std(dim='model').compute()\n",
    "\n",
    "delta_lithk_all.hvplot.image(\n",
    "    x='x', y='y', \n",
    "    clim=(0, 200), \n",
    "    cmap='gray_r',\n",
    "    clabel='Std dev of thickness change (m)'\n",
    ").opts(aspect='equal', title='Standard deviation of ice thickness change across models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa99615",
   "metadata": {},
   "source": [
    "### Computed scalars: mass above flotation\n",
    "\n",
    "As a further example, we will re-create a very small part of the analysis done for this paper:\n",
    "\n",
    "> Seroussi, H., Nowicki, S., Payne, A. J., Goelzer, H., Lipscomb, W. H., Abe-Ouchi, A., Agosta, C., Albrecht, T., Asay-Davis, X., Barthel, A., Calov, R., Cullather, R., Dumas, C., Galton-Fenzi, B. K., Gladstone, R., Golledge, N. R., Gregory, J. M., Greve, R., Hattermann, T., Hoffman, M. J., Humbert, A., Huybrechts, P., Jourdain, N. C., Kleiner, T., Larour, E., Leguy, G. R., Lowry, D. P., Little, C. M., Morlighem, M., Pattyn, F., Pelle, T., Price, S. F., Quiquet, A., Reese, R., Schlegel, N.-J., Shepherd, A., Simon, E., Smith, R. S., Straneo, F., Sun, S., Trusel, L. D., Van Breedam, J., van de Wal, R. S. W., Winkelmann, R., Zhao, C., Zhang, T., and Zwinger, T.: ISMIP6 Antarctica: a multi-model ensemble of the Antarctic ice sheet evolution over the 21st century, The Cryosphere, 14, 3033â€“3070, https://doi.org/10.5194/tc-14-3033-2020, 2020.\n",
    "\n",
    "(With results archived at https://zenodo.org/records/3940766)\n",
    "\n",
    "Specifically, we will calculate the summed volume above flotation for one model. In the cell below, we load the NetCDF files provided at the Zenodo archive above and plot them for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984020a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ivaf_reference = xr.open_dataset('external_data/ismip6_computed_scalars/computed_ivaf_AIS_JPL1_ISSM_exp05.nc', engine='h5netcdf', decode_times=False)\n",
    "tmp_ivaf_minus_ctrl_reference = xr.open_dataset('external_data/ismip6_computed_scalars/computed_ivaf_minus_ctrl_proj_AIS_JPL1_ISSM_exp05.nc', engine='h5netcdf', decode_times=False)\n",
    "\n",
    "ivaf_reference = xr.merge([tmp_ivaf_reference['ivaf'], tmp_ivaf_minus_ctrl_reference['ivaf'].rename('ivaf_minus_ctrl')])\n",
    "(ivaf_reference - ivaf_reference.isel(time=0)).hvplot.line(\n",
    "    x='time',\n",
    "    ylabel='Cumulative change in volume above flotation (m^3)',\n",
    "    title='Ice Volume Above Flotation for JPL1_ISSM exp05'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19b603",
   "metadata": {},
   "source": [
    "We need a few approximate constants (below) and then we also need to know the areal scaling factor of the EPSG:3031 projection, which is only true scale at -71 degrees south. To verify the result, we first compute it using PyProj and then validate it against the NetCDF file provided in the Zenodo link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658db49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_density = 917 # kg/m^3\n",
    "ocean_density = 1028 # kg/m^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5985ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scaling factor for each grid cell in regridded using pyproj\n",
    "# The scaling factor accounts for map projection distortion\n",
    "import pyproj\n",
    "\n",
    "# Create 2D grids for x and y\n",
    "xx, yy = np.meshgrid(comparison_grid.x.values, comparison_grid.y.values)\n",
    "\n",
    "# Set up the projection for EPSG:3031 (Antarctic Polar Stereographic)\n",
    "proj = pyproj.Proj('EPSG:3031')\n",
    "\n",
    "# Convert projected coordinates to lat/lon\n",
    "lons, lats = proj(xx, yy, inverse=True)\n",
    "\n",
    "# Get the factors at each grid point using the Proj object\n",
    "# get_factors returns: (meridional_scale, parallel_scale, areal_scale, \n",
    "#                       angular_distortion, meridian_parallel_angle, \n",
    "#                       meridian_convergence, tissot_semimajor, tissot_semiminor)\n",
    "# We want the areal_scale (index 2)\n",
    "factors = proj.get_factors(lons.ravel(), lats.ravel(), radians=False)\n",
    "areal_scale = factors.areal_scale.reshape(xx.shape)\n",
    "\n",
    "# Create the scaling factor as an xarray DataArray\n",
    "scale_factor = xr.DataArray(\n",
    "    areal_scale,\n",
    "    coords={'y': comparison_grid.y.values, 'x': comparison_grid.x.values},\n",
    "    dims=['y', 'x'],\n",
    "    name='scalefac',\n",
    "    attrs={\n",
    "        'long_name': 'Area scaling factor',\n",
    "        'description': 'Ratio of true area to projected area from pyproj.get_factors',\n",
    "        'units': '1',\n",
    "        'projection': 'EPSG:3031',\n",
    "    }\n",
    ")\n",
    "\n",
    "scale_factor.hvplot.image().opts(aspect='equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_file = xr.open_dataset('external_data/ismip6_computed_scalars/af2_el_ismip6_ant_01.nc')\n",
    "scale_file_interp = scale_file['af2'].interp(x=comparison_grid.x, y=comparison_grid.y, method='linear')\n",
    "\n",
    "(scale_file_interp - (1/scale_factor)).hvplot.image().opts(aspect='equal', clim=(-0.1, 0.1), title='Difference between computed and reference scaling factor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81961c81",
   "metadata": {},
   "source": [
    "Now we can define a function to find the volume above flotation and apply it to every model in our DataTree.\n",
    "\n",
    "The main syntax awkwardness here is having to check if we're on the right node level within the helper function:\n",
    "\n",
    "```python\n",
    "# Check if we're on a node that has the required variables\n",
    "if 'lithk' not in dt or 'base' not in dt or 'sftgrf' not in dt:\n",
    "    return dt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ivaf(dt):\n",
    "    # Check if we're on a node that has the required variables\n",
    "    if 'lithk' not in dt or 'base' not in dt or 'sftgrf' not in dt:\n",
    "        return dt\n",
    "    \n",
    "    lithk = dt['lithk']\n",
    "    base = dt['base']\n",
    "    sftgrf = dt['sftgrf']\n",
    "\n",
    "    resolution = dt['x'][1] - dt['x'][0]  # in meters\n",
    "\n",
    "    ivaf = ((lithk + ocean_density/ice_density * np.minimum(base, 0)) * sftgrf * (1/scale_factor)).sum(dim=['x', 'y']) * (resolution**2)  # in m^3\n",
    "    dt = dt.copy()\n",
    "    dt['ivaf'] = ivaf.rename('ivaf')\n",
    "    return dt\n",
    "\n",
    "regridded = regridded.map_over_datasets(calc_ivaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b0759",
   "metadata": {},
   "source": [
    "We've now setup the calculation for `ivaf` across every model, but not actually run it. For the plots below, we'll force computation of the volume above flotation for `exp05` and the same with the control run subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivaf = regridded['JPL1_ISSM']['exp05']['ivaf'].compute()\n",
    "ivaf_minus_ctrl = (ivaf - regridded['JPL1_ISSM']['ctrl_proj']['ivaf']).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = ((ivaf - ivaf.isel(time=0)).hvplot.line(x='time', label='exp05') *\n",
    "        (ivaf_minus_ctrl - ivaf_minus_ctrl.isel(time=0)).hvplot.line(x='time', label='exp05 - ctrl_proj')\n",
    "        ).opts(\n",
    "    ylabel='Cumulative change in\\nvolume above flotation (m^3)',\n",
    "    title='Computed changed in ice volume above flotation for JPL1_ISSM exp05',\n",
    "    legend_position='bottom_left', show_grid=True\n",
    "    )\n",
    "ref = (ivaf_reference - ivaf_reference.isel(time=0)).hvplot.line(\n",
    "    x='time',\n",
    "    ylabel='Cumulative change in\\nvolume above flotation (m^3)',\n",
    "    title='Reference (from doi.org/10.5281/zenodo.3940766)'\n",
    "    ).opts(legend_position='bottom_left', show_grid=True)\n",
    "\n",
    "calc + ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590d43c",
   "metadata": {},
   "source": [
    "On the left are the values we computed here, compared with the reference values on the right from the Zenodo archive. We computed ours on a resampled grid, so it's not an exact match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777805f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ismip-comparison-tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
